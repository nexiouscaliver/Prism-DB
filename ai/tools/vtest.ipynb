{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process SEC filing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Populate cik_ticker and submissions collections \n",
    "in company_eval db of mongodb for the 8 tickers specified. Submissions collection (table) only holds the dates of the submissions and not the actual filing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIK for nmr is 0001163653\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary functions from edgar_utils.py\n",
    "from db.edgar_utils import (\n",
    "    download_cik_ticker_map,\n",
    "    cik_from_ticker,\n",
    "    download_all_cik_submissions,\n",
    ")\n",
    "ticker = 'nmr'\n",
    "cik = cik_from_ticker(ticker)\n",
    "print(f'CIK for {ticker} is {cik}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary functions from edgar_utils.py\n",
    "from db.edgar_utils import download_cik_ticker_map, cik_from_ticker, download_all_cik_submissions\n",
    "\n",
    "# Define the tickers for the companies you're interested in\n",
    "tickers = [\"AAPL\",  \"META\", \"MSFT\", \"GOOG\", \"AMZN\", \"NVDA\", \"TSLA\" ]\n",
    "# tickers = [\"NMR\"]\n",
    "\n",
    "# Download the CIK-ticker map\n",
    "download_cik_ticker_map()\n",
    "\n",
    "# For each ticker, get the CIK and download all submissions\n",
    "for ticker in tickers:\n",
    "    cik = cik_from_ticker(ticker)\n",
    "    download_all_cik_submissions(cik)\n",
    "\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Get the latest filing documents and financial data for a given company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document already exists: https://www.sec.gov/Archives/edgar/data/0000320193/000032019324000123/aapl-20240928.htm\n",
      "Document already exists: https://www.sec.gov/Archives/edgar/data/0000320193/000032019325000008/aapl-20241228.htm\n",
      "submissions file not found in mongodb for 0001666138\n",
      "2025-01-30 (10-K): https://www.sec.gov/Archives/edgar/data/0001326801/000132680125000017/meta-20241231.htm\n",
      "10-K downloaded successfully\n",
      "Closest '10-K' document downloaded successfully.\n",
      "2024-07-30 (10-K): https://www.sec.gov/Archives/edgar/data/0000789019/000095017024087843/msft-20240630.htm\n",
      "10-K downloaded successfully\n",
      "Closest '10-K' document downloaded successfully.\n",
      "2025-01-29 (10-Q): https://www.sec.gov/Archives/edgar/data/0000789019/000095017025010491/msft-20241231.htm\n",
      "10-Q downloaded successfully\n",
      "Closest '10-Q' document downloaded successfully.\n",
      "2025-02-05 (10-K): https://www.sec.gov/Archives/edgar/data/0001652044/000165204425000014/goog-20241231.htm\n",
      "10-K downloaded successfully\n",
      "Closest '10-K' document downloaded successfully.\n",
      "2025-02-07 (10-K): https://www.sec.gov/Archives/edgar/data/0001018724/000101872425000004/amzn-20241231.htm\n",
      "10-K downloaded successfully\n",
      "Closest '10-K' document downloaded successfully.\n",
      "2024-02-21 (10-K): https://www.sec.gov/Archives/edgar/data/0001045810/000104581024000029/nvda-20240128.htm\n",
      "10-K downloaded successfully\n",
      "Closest '10-K' document downloaded successfully.\n",
      "2024-11-20 (10-Q): https://www.sec.gov/Archives/edgar/data/0001045810/000104581024000316/nvda-20241027.htm\n",
      "10-Q downloaded successfully\n",
      "Closest '10-Q' document downloaded successfully.\n",
      "2025-01-30 (10-K): https://www.sec.gov/Archives/edgar/data/0001318605/000162828025003063/tsla-20241231.htm\n",
      "10-K downloaded successfully\n",
      "Closest '10-K' document downloaded successfully.\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from db.edgar_utils import download_latest_filings, download_financial_data, cik_from_ticker\n",
    "\n",
    "# Define the tickers for the companies you're interested in\n",
    "tickers = [\"AAPL\", \"ATKR\", \"META\", \"MSFT\", \"GOOG\", \"AMZN\", \"NVDA\", \"TSLA\"]\n",
    "# tickers = [ \"NMR\"]\n",
    "\n",
    "for ticker in tickers:\n",
    "    filings = download_latest_filings(ticker)\n",
    "    \n",
    "    # Now get the financial data for the company\n",
    "    cik = cik_from_ticker(ticker)\n",
    "    download_financial_data(cik)\n",
    "    \n",
    "    # print(filings)\n",
    "    \n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Parse Document\n",
    "\n",
    "    This gets documents from documents collection and parses it. Parsing stores each sections oject with section titile and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to parse filings for AAPL...\n",
      "Done parsing filings for AAPL\n",
      "Starting to parse filings for ATKR...\n",
      "No 10-K document found for cik 0001666138\n",
      "Done parsing filings for ATKR\n",
      "Starting to parse filings for META...\n",
      "form type: \t\t10-K\n",
      "cik                   0001326801\n",
      "name        Meta Platforms, Inc.\n",
      "ticker                      META\n",
      "exchange                  Nasdaq\n",
      "Name: 5, dtype: object\n",
      "No 10-Q document found for cik 0001326801 after the 10-K document\n",
      "Done parsing filings for META\n",
      "Starting to parse filings for MSFT...\n",
      "form type: \t\t10-K\n",
      "cik             0000789019\n",
      "name        MICROSOFT CORP\n",
      "ticker                MSFT\n",
      "exchange            Nasdaq\n",
      "Name: 2, dtype: object\n",
      "form type: \t\t10-Q\n",
      "cik             0000789019\n",
      "name        MICROSOFT CORP\n",
      "ticker                MSFT\n",
      "exchange            Nasdaq\n",
      "Name: 2, dtype: object\n",
      "Done parsing filings for MSFT\n",
      "Starting to parse filings for GOOG...\n",
      "form type: \t\t10-K\n",
      "cik            0001652044\n",
      "name        Alphabet Inc.\n",
      "ticker              GOOGL\n",
      "exchange           Nasdaq\n",
      "Name: 4, dtype: object\n",
      "No 10-Q document found for cik 0001652044 after the 10-K document\n",
      "Done parsing filings for GOOG\n",
      "Starting to parse filings for AMZN...\n",
      "form type: \t\t10-K\n",
      "cik             0001018724\n",
      "name        AMAZON COM INC\n",
      "ticker                AMZN\n",
      "exchange            Nasdaq\n",
      "Name: 3, dtype: object\n",
      "No 10-Q document found for cik 0001018724 after the 10-K document\n",
      "Done parsing filings for AMZN\n",
      "Starting to parse filings for NVDA...\n",
      "form type: \t\t10-K\n",
      "cik          0001045810\n",
      "name        NVIDIA CORP\n",
      "ticker             NVDA\n",
      "exchange         Nasdaq\n",
      "Name: 1, dtype: object\n",
      "form type: \t\t10-Q\n",
      "cik          0001045810\n",
      "name        NVIDIA CORP\n",
      "ticker             NVDA\n",
      "exchange         Nasdaq\n",
      "Name: 1, dtype: object\n",
      "Done parsing filings for NVDA\n",
      "Starting to parse filings for TSLA...\n",
      "form type: \t\t10-K\n",
      "cik          0001318605\n",
      "name        Tesla, Inc.\n",
      "ticker             TSLA\n",
      "exchange         Nasdaq\n",
      "Name: 6, dtype: object\n",
      "No 10-Q document found for cik 0001318605 after the 10-K document\n",
      "Done parsing filings for TSLA\n"
     ]
    }
   ],
   "source": [
    "from db.edgar_utils import cik_from_ticker\n",
    "from db.utils import parse_latest_filings\n",
    "\n",
    "tickers = [\"AAPL\", \"ATKR\", \"META\", \"MSFT\", \"GOOG\", \"AMZN\", \"NVDA\", \"TSLA\"]\n",
    "# tickers = [\"CUK\"]\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        print(f\"Starting to parse filings for {ticker}...\")\n",
    "        cik = cik_from_ticker(ticker)\n",
    "        if cik:  # Check if a CIK was successfully retrieved\n",
    "            parse_latest_filings(cik)\n",
    "            print(f\"Done parsing filings for {ticker}\")\n",
    "        else:\n",
    "            print(f\"CIK not found for {ticker}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse filings for {ticker}. Error: {e}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 Alternative way for Parsing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db.edgar_utils import cik_from_ticker\n",
    "from db.utils import parse_document\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Create a MongoClient to the running MongoDB instance\n",
    "client = MongoClient('localhost', 27017)\n",
    "# Access your database\n",
    "db = client['company_eval']\n",
    "\n",
    "tickers = [\"AAPL\", \"BABA\", \"ATKR\", \"META\", \"MSFT\", \"GOOG\", \"AMZN\", \"NVDA\"]\n",
    "filing_types = [\"10-K\", \"10-Q\", \"8-K\"]\n",
    "\n",
    "for ticker in tickers:\n",
    "\tcik = cik_from_ticker(ticker)\n",
    "\n",
    "\tfor filing_type in filing_types:\n",
    "\t\t# Get all documents for a given cik and filing type, sorted by filing date in descending order\n",
    "\t\tquery = {\"cik\": cik, \"form_type\": filing_type}\n",
    "\t\tdoc_cursor = db[\"documents\"].find(\n",
    "\t\t\tquery,\n",
    "\t\t\t{\"_id\": 1, \"form_type\": 1, \"filing_date\": 1, \"cik\": 1, \"html\": 1}\n",
    "\t\t).sort(\"filing_date\", -1)\n",
    "\n",
    "\t\t# Efficiently check if any documents were found\n",
    "\t\tfirst_doc = next(doc_cursor, None)\n",
    "\t\tif first_doc:\n",
    "\t\t\t# Since we've already fetched the first document, process it\n",
    "\t\t\tprint(f\"Parsing {filing_type} document for ticker {ticker}\")\n",
    "\t\t\tparse_document(first_doc)\n",
    "\t\t\t# Iterate over the rest of the documents and parse them\n",
    "\t\t\tfor doc in doc_cursor:\n",
    "\t\t\t\tprint(f\"Parsing {filing_type} document for ticker {ticker}\")\n",
    "\t\t\t\tparse_document(doc)\n",
    "\t\telse:\n",
    "\t\t\tprint(f\"No {filing_type} document found for ticker {ticker}\")\n",
    "\n",
    "print(\"Processing completed for all tickers and filing types.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Call Risk Memo Tool to generate Risk Memo\n",
    "This will generate risk memo in pdf format in save it generated_files folder in the project root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai.tools.risk_memo_tool import RiskMemoTool\n",
    "\n",
    "risk_memo_tool = RiskMemoTool()\n",
    "response = risk_memo_tool.generate_risk_memo(\"AAPL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get named section from parsed document using ticker in _id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from db.utils import get_section_text\n",
    "\n",
    "# Example usage\n",
    "ticker =\"AAPL\"\n",
    "section = \"risk factors\"\n",
    "response_json = get_section_text(ticker, section)\n",
    "\n",
    "# Convert the JSON string back to a dictionary\n",
    "response_dict = json.loads(response_json)\n",
    "\n",
    "# Accessing the newly formatted response\n",
    "print(f\"Ticker: {response_dict['ticker']}\")\n",
    "print(f\"Section Name: {response_dict['section_name']}\")\n",
    "if response_dict['section_text']:\n",
    "    print(f\"Text for section '{response_dict['section_name']}':\\n{response_dict['section_text']}\\n\")\n",
    "else:\n",
    "    print(\"No section text found.\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access MongoDB to check\n",
    "The following code will show you contents of a collection (table) in the MongoDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define DB connection\n",
    "# from pymongo import MongoClient\n",
    "# # Create a MongoClient to the running MongoDB instance\n",
    "# client = MongoClient('localhost', 27017)\n",
    "# # Access your database\n",
    "# db = client['company_eval']\n",
    "\n",
    "# # Access the 'submissions' collection\n",
    "# submissions = db['submissions']\n",
    "\n",
    "# # Query all documents in the collection\n",
    "# # for doc in submissions.find().limit(8):\n",
    "# #     print(doc)\n",
    "\n",
    "# # find fields in documents collection\n",
    "# # doc = db[\"documents\"].find_one()\n",
    "# # print(doc.keys())\n",
    "\n",
    "\n",
    "# gets a url for the latest 10-K form for a company\n",
    "# docs = db[\"documents\"].find({\"_id\": {\"$regex\": \"nvda\"}, \"form_type\": \"10-K\"}, {\"_id\": 1}).sort(\"_id\", -1).limit(1)\n",
    "# for doc in docs:\n",
    "#     print(doc)\n",
    "\n",
    "# gets the section text for a document\n",
    "# doc = db[\"parsed_documents\"].find_one({\"form_type\":\"10-K\"})\n",
    "# if doc and 'sections' in doc:\n",
    "#     for section, text in doc['sections'].items():\n",
    "#         print(f\"Section: {section}\")\n",
    "#         print(f\"Text: {text}\")\n",
    "\n",
    "from db.edgar_utils import company_from_ticker\n",
    "\n",
    "\n",
    "ticker = \"aapl\"\n",
    "company = company_from_ticker(ticker)\n",
    "print(company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test sec_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from ai.tools.sec import SecTool  \n",
    "import asyncio\n",
    "\n",
    "# Initialize the SecTool object\n",
    "sec_tool = SecTool()\n",
    "\n",
    "# Define an async function to call the method\n",
    "async def get_section():\n",
    "    # Call the get_filing_section_in_kb method with \"AAPL\" and \"Risk Factors\" and await its result\n",
    "    result = await sec_tool.get_filing_section_in_kb(\"AAPL\", \"risk factors\")\n",
    "    return result\n",
    "\n",
    "# Run the async function and get the result\n",
    "result = await get_section()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Get all sections from risk_memos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db.utils import get_all_sections_from_risk_memo\n",
    "\n",
    "sections = get_all_sections_from_risk_memo(\"AAPL\")\n",
    "\n",
    "print(sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test process_sections (includes db inserts_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai.tools.process_risk_memo import process_sections\n",
    "\n",
    "\n",
    "\n",
    "respoonse = process_sections(\"AAPL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test get sections from risk memo\n",
    "Once we get ALL the available sections then these sections together as one are sent to Assistant. \n",
    "Assistant then generates final draft of the risk memo as a single document.\n",
    "This document is then saved to the MOngodb - risk_memos under section risk_memo_final_draft. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db.utils import (\n",
    "    get_all_sections_from_risk_memo, \n",
    "    get_field_from_risk_memo, \n",
    "    add_section_to_risk_memo,\n",
    "    get_section_text\n",
    "    )\n",
    "    \n",
    "# from ai.tools.process_risk_memo import RiskMemoGenerator\n",
    "\n",
    "# risk_memo_generator = RiskMemoGenerator()\n",
    "\n",
    "# from ai.risk_memo_drafter import memo_draft_assistant\n",
    "\n",
    "ticker = \"AAPL\"\n",
    "# Get section text\n",
    "section_text = get_section_text(ticker, \"profile\")\n",
    "print(section_text)\n",
    "\n",
    "# Get all summerised sections from the risk memo\n",
    "# risk_memo_sections = get_all_sections_from_risk_memo(ticker)\n",
    "# print(risk_memo_sections)\n",
    "\n",
    "# company = get_field_from_risk_memo(ticker, \"companyName\")\n",
    "# print(company)\n",
    "\n",
    "# # Generate the final draft of the risk memo\n",
    "# draft_memo = risk_memo_generator.draft_risk_memo(risk_memo_sections)\n",
    "# print(draft_memo)\n",
    "\n",
    "\n",
    "# # Save the final draft in the risk_memos collection of mongoDB\n",
    "# add_section_response = add_section_to_risk_memo(ticker, ticker, \"risk_memo_final_draft\", draft_memo)\n",
    "\n",
    "# print(add_section_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db.utils import add_section_to_risk_memo\n",
    "\n",
    "\n",
    "# Test data\n",
    "ticker = \"AAPL\"\n",
    "section_name = \"financials\"\n",
    "text = '{\"text\": \"This is the financial section content.\"}'\n",
    "\n",
    "# Call the function with test data\n",
    "result = add_section_to_risk_memo(ticker, section_name, text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Sections (LLm call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_risk_memo import  process_sections\n",
    "\n",
    "response = process_sections(\"TSLA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the final PDF generation and its various functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Create PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai.tools.risk_memo_pdf_generator import RiskMemoGenerator\n",
    "\n",
    "generator = RiskMemoGenerator()\n",
    "file_name = generator.create_pdf(\"MSFT\")\n",
    "\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from market_data import get_company_profile, get_financial_overview\n",
    "\n",
    "# response = get_company_profile(\"AAPL\")\n",
    "\n",
    "response = get_financial_overview(\"MSFT\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Google Document AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting form fields:\n",
      "\n",
      "Found 6 form field(s) on page 1:\n",
      "    * 'Effective Date:': '6/1/2023'\n",
      "    * '4) How many years has Assured been in business:': '6'\n",
      "    * 'Diving from shore/fixed objects in navigable waters:': '20 %'\n",
      "    * 'Diving in non-navigable waters:': '0 %'\n",
      "    * 'Part Time': '_2_'\n",
      "    * 'Submission 8': '8 Street Rd\\nLong Beach, CA 90808'\n",
      "\n",
      "Found 3 form field(s) on page 3:\n",
      "    * 'Mixed gas diving:': '39 (nitrox)%'\n",
      "    * 'Shallow air diving:': '60%'\n",
      "    * \"Deep air diving (over 130' fsw):\": '1%'\n",
      "\n",
      "Found 1 form field(s) on page 4:\n",
      "    * 'Diver/\\nDive Tender/\\nDive Supervisor\\nPayroll': '$30,000'\n",
      "\n",
      "Extracting entities:\n",
      "    * 'generic_entities': '' (0.0% confident)\n",
      "    * 'generic_entities': '' (0.0% confident)\n",
      "    * 'generic_entities': '' (0.0% confident)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "\n",
    "# ====== Configuration ======\n",
    "\n",
    "# Path to your service account key JSON file\n",
    "service_key_path = \"/home/rhythm/Documents/Code/regenai/backend/credentials/regenai-service-key.json\"\n",
    "\n",
    "# Replace the following placeholders with your actual values\n",
    "project_id = \"regenai\"                       # Your Google Cloud project ID\n",
    "location = \"eu\"                              # Processor location, e.g., \"us\", \"eu\"\n",
    "processor_id = \"41265319209e4d75\"            # Your Document AI processor ID\n",
    "processor_version_id = None                  # Optional: Specify if using a specific processor version\n",
    "pdf_file_path = \"vtest8.pdf\"                 # Path to the PDF file you want to process\n",
    "\n",
    "# =============================\n",
    "\n",
    "# Step 1: Set the environment variable for Google Application Credentials\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = service_key_path\n",
    "\n",
    "# Step 2: Verify that the credentials file exists\n",
    "if not os.path.isfile(service_key_path):\n",
    "    raise FileNotFoundError(f\"Credentials file not found at: {service_key_path}\")\n",
    "\n",
    "# Step 3: Initialize the Document AI client\n",
    "client_options = {\"api_endpoint\": f\"{location}-documentai.googleapis.com\"}\n",
    "client = documentai.DocumentProcessorServiceClient(client_options=client_options)\n",
    "\n",
    "# Step 4: Define the processor name\n",
    "if processor_version_id:\n",
    "    # Full resource name of the processor version\n",
    "    processor_name = client.processor_version_path(project_id, location, processor_id, processor_version_id)\n",
    "else:\n",
    "    # Full resource name of the processor\n",
    "    processor_name = client.processor_path(project_id, location, processor_id)\n",
    "\n",
    "# Step 5: Verify that the PDF file exists\n",
    "if not os.path.isfile(pdf_file_path):\n",
    "    raise FileNotFoundError(f\"PDF file not found at: {pdf_file_path}\")\n",
    "\n",
    "# Step 6: Read the PDF file into memory\n",
    "with open(pdf_file_path, \"rb\") as file:\n",
    "    pdf_content = file.read()\n",
    "\n",
    "# Step 7: Create a RawDocument\n",
    "raw_document = documentai.RawDocument(content=pdf_content, mime_type=\"application/pdf\")\n",
    "\n",
    "# Step 8: Create the process request\n",
    "request = documentai.ProcessRequest(\n",
    "    name=processor_name,\n",
    "    raw_document=raw_document,\n",
    ")\n",
    "\n",
    "# Step 9: Process the document\n",
    "result = client.process_document(request=request)\n",
    "\n",
    "# Step 10: Retrieve the document object\n",
    "document = result.document\n",
    "\n",
    "# # Helper function to extract text from layout\n",
    "# def layout_to_text(layout, text):\n",
    "#     \"\"\"Extracts text from the document based on the layout.\"\"\"\n",
    "#     if not layout.text_anchor.text_segments:\n",
    "#         return \"\"\n",
    "#     text_chunks = []\n",
    "#     for segment in layout.text_anchor.text_segments:\n",
    "#         start_index = segment.start_index if segment.start_index is not None else 0\n",
    "#         end_index = segment.end_index if segment.end_index is not None else len(text)\n",
    "#         text_chunks.append(text[start_index:end_index])\n",
    "#     return ''.join(text_chunks)\n",
    "\n",
    "def layout_to_text(layout, text):\n",
    "    \"\"\"\n",
    "    Extracts text from the document based on the layout, handling multiple text segments.\n",
    "    \"\"\"\n",
    "    if not layout.text_anchor.text_segments:\n",
    "        return \"\"\n",
    "    # Concatenate all text segments for the layout\n",
    "    return \"\".join(\n",
    "        text[int(segment.start_index) : int(segment.end_index)]\n",
    "        for segment in layout.text_anchor.text_segments\n",
    "    )\n",
    "\n",
    "# ====== Extract and Print Form Fields ======\n",
    "\n",
    "print(\"\\nExtracting form fields:\")\n",
    "for page in document.pages:\n",
    "    if not page.form_fields:\n",
    "        continue  # Skip pages without form fields\n",
    "    print(f\"\\nFound {len(page.form_fields)} form field(s) on page {page.page_number}:\")\n",
    "    for field in page.form_fields:\n",
    "        name = layout_to_text(field.field_name, document.text)\n",
    "        value = layout_to_text(field.field_value, document.text)\n",
    "        print(f\"    * {repr(name.strip())}: {repr(value.strip())}\")\n",
    "\n",
    "# ====== Extract and Print Entities ======\n",
    "\n",
    "print(\"\\nExtracting entities:\")\n",
    "if not document.entities:\n",
    "    print(\"No entities found in the document.\")\n",
    "else:\n",
    "    for entity in document.entities:\n",
    "        key = entity.type_\n",
    "        text_value = layout_to_text(entity, document.text)\n",
    "        confidence = entity.confidence\n",
    "        normalized_value = entity.normalized_value.text if entity.normalized_value else \"\"\n",
    "        print(f\"    * {repr(key)}: {repr(text_value.strip())} ({confidence:.1%} confident)\")\n",
    "        if normalized_value:\n",
    "            print(f\"      Normalized Value: {repr(normalized_value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"form_fields\": {\n",
      "    \"Yes\": \"☑\",\n",
      "    \"No\": \"☑\",\n",
      "    \"MEL Application\": \"020121\",\n",
      "    \"ACORD Workers' Compensation Application\": \"\",\n",
      "    \"Minimum 4 years and currently valued Loss Runs\": \"\",\n",
      "    \"Description of operations\": \"\",\n",
      "    \"City\": \"EHT\",\n",
      "    \"W.C.:\": \"500,000\\n$\",\n",
      "    \"L.S.H.W.A.:\": \"0\\n$\",\n",
      "    \"Total gross annual payroll: $\": \"700,000\",\n",
      "    \"Jones Act:\": \"$\",\n",
      "    \"5. How many years has Insured been in operation?\": \"5\",\n",
      "    \"Street\": \"2 Street Rd\\n:\",\n",
      "    \"6. Full details of Insured's overwater operations?\": \"Soil boring from non-owned barge\",\n",
      "    \"State\": \"NJ\",\n",
      "    \"Zip\": \"08234\",\n",
      "    \"1. Full Name of Insured:\": \"Submission 2\",\n",
      "    \"EHT\": \"City\",\n",
      "    \"N\\nNo\": \"\",\n",
      "    \"No\\n은\": \"\",\n",
      "    \"No\\nN\": \"☑\",\n",
      "    \"Expiring Date\": \"02/24/2023\",\n",
      "    \"Limits carried\": \"$ 1,000,000\",\n",
      "    \"Limit Required\": \"$1,000,000\",\n",
      "    \"Premium Charged\": \"$ 35,000\"\n",
      "  },\n",
      "  \"entities\": {\n",
      "    \"generic_entities\": [\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ]\n",
      "  },\n",
      "  \"tables\": [\n",
      "    [],\n",
      "    [\n",
      "      {\n",
      "        \"Limits carried\": \"Expiring Date\",\n",
      "        \"$ 1,000,000\": \"02/24/2023\"\n",
      "      },\n",
      "      {\n",
      "        \"Limits carried\": \"Premium Charged\",\n",
      "        \"$ 1,000,000\": \"$ 35,000\"\n",
      "      },\n",
      "      {\n",
      "        \"Limits carried\": \"Limit Required\",\n",
      "        \"$ 1,000,000\": \"$1,000,000\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"text\": \"Submission Requirements:\\nACORD Workers' Compensation Application\\nMinimum 4 years and currently valued Loss Runs\\nDescription of operations\\nMEL Application\\nMEL Application\\n1. Full Name of Insured: Submission 2\\n2 Street Rd\\n2. Physical Address:\\nEHT\\nNJ\\n08234\\nStreet\\nCity\\nState Zip\\n3. Insured Email Address:\\n4. Telephone:\\nFax:\\n5\\n5. How many years has Insured been in operation?\\n6. Full details of Insured's overwater operations? Soil boring from non-owned barge\\n7. Total number of employees:\\nTotal gross annual payroll: $ 700,000\\n4\\n8. Total number of employees exposed overwater per annum:\\n4\\n9. Total payroll for employees exposed overwater:\\n4\\n10. Maximum number of employees exposed overwater at any one time:\\n11. Gross payroll split for last 12 months:\\nJones Act: $\\n0\\nL.S.H.W.A.: $\\n500,000\\nW.C.: $\\n12. Gross split for next 12 months:\\nJones Act: $15,000\\nL.S.H.W.A.: $15,000\\nW.C.: $ 570,000\\n(Underwriters reserve the right to audit the Insured's accounts at any time, at Underwriters' expense)\\n13. Does the Insured engage in any diving operations?\\nYes ☑ No\\nIf yes: # of divers exposed at any one time\\nAnd, # of tenders exposed at any one time\\nDo tenders dive?\\nYes ☑ No\\n14. Does the Insured own and/or operate any *watercraft?\\nYes\\nNo\\nMEL Application 020121\\nPage 1 of 2\\nRSGprograms.com\\nPlease provide full details:\\n15. Do/will employees work on or from or have any connection with *watercraft during the\\npolicy period?\\n☑ Yes\\nNo\\n16. Is watercraft work done dockside and/or in Insured's yard only?\\nYes ☑ No\\n은 은은\\n17. If shipbuilding/ship repair do employees do trial trips?\\nYes\\nNo\\nIf so, how often and time involved per annum:\\n18. If employees work on or from or have any connection with watercraft away from dockside, does\\nany one employee spend more than 25% of his/her time working on or from or in connection with\\nwatercraft?\\nYes ☑ No\\nN N\\n19. Does/will the Insured have jobs of short duration overwater?\\n☑ Yes\\nNo\\nIf so, please provide the maximum percentage of time during the job that any one employee will be\\nworking on or from the or in connection with the watercraft: 90\\n%\\n20. Do/will employees keep any of their tools or equipment on watercraft?\\n☑ Yes\\nNo\\n21. Full 5 year death/injury/illness record including any reserves (including any claim/incident arising Overwater reported\\nto Workers' Compensation and/or L.S.H.W.A. Insurers), use separate sheet necessary:\\n22. Present Insurers:\\nLimits carried\\n$ 1,000,000\\nExpiring Date\\n02/24/2023\\nPremium Charged\\n$ 35,000\\nLimit Required\\n$1,000,000\\n*Note: The definition of a watercraft includes any vessel or special structure other than a fixed, permanent platform, which\\nis capable of navigation either under its own power or being towed. Jack-ups, semi-submersibles and/or other barges are\\ndeemed to be watercraft for the purpose of the above questions.\\nIMPORTANT:\\nTHIS QUESTIONNAIRE IS TO BE COMPLETED AND SIGNED BY THE INSURED AND WILL FORM PART OF THE MARITIME\\nEMPLOYER'S LIABILITY POLICY ISSUED. THE PREMIUM CHARGED AND THE CONDITIONS OF THIS POLICY ARE BASED UPON\\nTHE INFORMATION PROVIDED IN THIS QUESTIONNAIRE. ANY OPERATIONAL AND/OR PHYSICAL CHANGES IN THE NATURE\\nOF THE INSURED'S OVERWATER OPERATION DURING THE POLICY PERIOD WHICH MATERIALLY CHANGES OR ALTERS IN ANY\\nWAY THE INFORMATION CONTAINED IN THIS QUESTIONNAIRE MUST IMMEDIATELY BE ADVISED TO UNDERWRITERS. ANY\\nCHANGES ADVISED WILL BE ASSESSED BY UNDERWRITERS TO ENABLE THEM TO DECIDE WHETHER THEY ARE PREPARED TO\\nCONTINUE TO PROVIDE THIS COVERAGE AND AT WHAT TERMS. FAILURE TO COMPLY WITH THIS REQUIREMENT WILL VOID\\nTHE POLICY.\\nAPPLICANT SIGNATURE\\nDATE\\nPRINT NAME\\nThe description of this program is only a summary of available coverages. Actual policy language will dictate the scope of coverage in the event of a claim. We encourage policyholders and their\\nagents to read the full policy form and any applicable endorsements for full terms and conditions. Effective September 1, 2020, Ryan Specialty Group created RSG National Specialty Programs,\\nwhich took over operations that were formerly the All Risks National Specialty Programs. Thirty-year industry veteran, Chris McGovern continues to manage 25+ distinct RSG National Specialty\\nPrograms. RSG National Specialty Programs is a unit of the RSG Underwriting Managers division of RSG Specialty, LLC, a Delaware limited liability company based in Illinois. RSG Specialty, LLC, is a\\nsubsidiary of Ryan Specialty Group, LLC (RSG). RSG National Specialty Programs works directly with brokers, agents and insurance carriers, and as such does not solicit insurance from the public.\\nSome products may only be available in certain states, and some products may only be available from surplus lines insurers. In California: RSG Specialty Insurance Services, LLC (License #\\n0G97516). ©2021 Ryan Specialty Group, LLC\\nMEL Application 020121\\nPage 2 of 2\\nRSGprograms.com\\n\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "# ====== Configuration ======\n",
    "\n",
    "# Path to your service account key JSON file\n",
    "service_key_path = \"/home/rhythm/Documents/Code/regenai/backend/credentials/regenai-service-key.json\"\n",
    "\n",
    "# Replace the following placeholders with your actual values\n",
    "project_id = \"regenai\"                       # Your Google Cloud project ID\n",
    "location = \"eu\"                              # Processor location, e.g., \"us\", \"eu\"\n",
    "processor_id = \"41265319209e4d75\"            # Your Document AI processor ID\n",
    "processor_version_id = None                  # Optional: Specify if using a specific processor version\n",
    "pdf_file_path = \"vtest2b.pdf\"                 # Path to the PDF file you want to process\n",
    "\n",
    "# =============================\n",
    "\n",
    "# Set the environment variable for Google Application Credentials\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = service_key_path\n",
    "\n",
    "# Verify that the credentials file exists\n",
    "if not os.path.isfile(service_key_path):\n",
    "    raise FileNotFoundError(f\"Credentials file not found at: {service_key_path}\")\n",
    "\n",
    "# Initialize the Document AI client\n",
    "client_options = {\"api_endpoint\": f\"{location}-documentai.googleapis.com\"}\n",
    "client = documentai.DocumentProcessorServiceClient(client_options=client_options)\n",
    "\n",
    "# Define the processor name\n",
    "if processor_version_id:\n",
    "    # Full resource name of the processor version\n",
    "    processor_name = client.processor_version_path(project_id, location, processor_id, processor_version_id)\n",
    "else:\n",
    "    # Full resource name of the processor\n",
    "    processor_name = client.processor_path(project_id, location, processor_id)\n",
    "\n",
    "# Verify that the PDF file exists\n",
    "if not os.path.isfile(pdf_file_path):\n",
    "    raise FileNotFoundError(f\"PDF file not found at: {pdf_file_path}\")\n",
    "\n",
    "# Read the PDF file into memory\n",
    "with open(pdf_file_path, \"rb\") as file:\n",
    "    pdf_content = file.read()\n",
    "\n",
    "# Create a RawDocument\n",
    "raw_document = documentai.RawDocument(content=pdf_content, mime_type=\"application/pdf\")\n",
    "\n",
    "# Create the process request\n",
    "request = documentai.ProcessRequest(\n",
    "    name=processor_name,\n",
    "    raw_document=raw_document,\n",
    ")\n",
    "\n",
    "# Process the document\n",
    "result = client.process_document(request=request)\n",
    "\n",
    "# Retrieve the document object\n",
    "document = result.document\n",
    "\n",
    "# Helper function to extract text from layout\n",
    "def layout_to_text(layout, text):\n",
    "    \"\"\"\n",
    "    Extracts text from the document based on the layout, handling multiple text segments.\n",
    "    \"\"\"\n",
    "    if not layout.text_anchor.text_segments:\n",
    "        return \"\"\n",
    "    # Concatenate all text segments for the layout\n",
    "    return \"\".join(\n",
    "        text[int(segment.start_index) : int(segment.end_index)]\n",
    "        for segment in layout.text_anchor.text_segments\n",
    "    )\n",
    "\n",
    "# Initialize the dictionary to hold the extracted data\n",
    "extracted_data = {\n",
    "    \"form_fields\": {},\n",
    "    \"entities\": {},\n",
    "    \"tables\": [],\n",
    "    \"text\": \"\"\n",
    "}\n",
    "\n",
    "# Extract Form Fields\n",
    "for page in document.pages:\n",
    "    for field in page.form_fields:\n",
    "        field_name = layout_to_text(field.field_name, document.text).strip()\n",
    "        field_value = layout_to_text(field.field_value, document.text).strip()\n",
    "        extracted_data[\"form_fields\"][field_name] = field_value\n",
    "\n",
    "# Extract Entities\n",
    "for entity in document.entities:\n",
    "    entity_type = entity.type_.strip()\n",
    "    entity_text = layout_to_text(entity, document.text).strip()\n",
    "    # Handle multiple entities of the same type\n",
    "    if entity_type in extracted_data[\"entities\"]:\n",
    "        if isinstance(extracted_data[\"entities\"][entity_type], list):\n",
    "            extracted_data[\"entities\"][entity_type].append(entity_text)\n",
    "        else:\n",
    "            extracted_data[\"entities\"][entity_type] = [extracted_data[\"entities\"][entity_type], entity_text]\n",
    "    else:\n",
    "        extracted_data[\"entities\"][entity_type] = entity_text\n",
    "\n",
    "# Extract Tables\n",
    "for page in document.pages:\n",
    "    for table in page.tables:\n",
    "        table_data = []\n",
    "        # Extract header cells\n",
    "        headers = []\n",
    "        if table.header_rows:\n",
    "            for header_cell in table.header_rows[0].cells:\n",
    "                header_text = layout_to_text(header_cell.layout, document.text).strip()\n",
    "                headers.append(header_text)\n",
    "        # Extract body cells\n",
    "        for row in table.body_rows:\n",
    "            row_data = {}\n",
    "            for idx, cell in enumerate(row.cells):\n",
    "                cell_text = layout_to_text(cell.layout, document.text).strip()\n",
    "                if headers and idx < len(headers):\n",
    "                    row_data[headers[idx]] = cell_text\n",
    "                else:\n",
    "                    row_data[f\"column_{idx+1}\"] = cell_text\n",
    "            table_data.append(row_data)\n",
    "        extracted_data[\"tables\"].append(table_data)\n",
    "\n",
    "# Extract Full Text\n",
    "extracted_data[\"text\"] = document.text\n",
    "\n",
    "# Optionally, save extracted data to JSON file\n",
    "with open('extracted_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(extracted_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Print the extracted data (optional)\n",
    "print(json.dumps(extracted_data, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Date\": \"2024-12-14\",\n",
      "  \"Insured\": \"Submission 2, EHT, NJ, 08234, Street\",\n",
      "  \"Effective Start Date\": \"2024-12-14\",\n",
      "  \"Effective End Date\": \"2025-12-14\",\n",
      "  \"Deductible\": \"$5000\",\n",
      "  \"Premium Rate\": \"\",\n",
      "  \"Estimated Annual Payroll\": \"700000\",\n",
      "  \"Policyholder/ Applicant\\u2019s Printed Name\": \"Submission 2\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# OCR text provided (truncated for brevity)\n",
    "ocr_text = \"\"\"\n",
    "Submission Requirements:\n",
    "ACORD Workers' Compensation Application\n",
    "Minimum 4 years and currently valued Loss Runs\n",
    "Description of operations\n",
    "MEL Application\n",
    "MEL Application\n",
    "1. Full Name of Insured: Submission 2\n",
    "2 Street Rd\n",
    "2. Physical Address:\n",
    "EHT\n",
    "NJ\n",
    "08234\n",
    "Street\n",
    "City\n",
    "State Zip\n",
    "3. Insured Email Address:\n",
    "4. Telephone:\n",
    "Fax:\n",
    "5\n",
    "5. How many years has Insured been in operation?\n",
    "6. Full details of Insured's overwater operations? Soil boring from non-owned barge\n",
    "7. Total number of employees:\n",
    "Total gross annual payroll: $ 700,000\n",
    "4\n",
    "8. Total number of employees exposed overwater per annum:\n",
    "4\n",
    "9. Total payroll for employees exposed overwater:\n",
    "4\n",
    "10. Maximum number of employees exposed overwater at any one time:\n",
    "11. Gross payroll split for last 12 months:\n",
    "Jones Act: $\n",
    "0\n",
    "L.S.H.W.A.: $\n",
    "500,000\n",
    "W.C.: $\n",
    "12. Gross split for next 12 months:\n",
    "Jones Act: $15,000\n",
    "L.S.H.W.A.: $15,000\n",
    "W.C.: $ 570,000\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "# Define a function to extract information based on possible field names\n",
    "def extract_field(text, field_names, after_colon=True):\n",
    "    pattern = r'(?:' + '|'.join(re.escape(field) for field in field_names) + r')\\s*:?(.+)'\n",
    "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "    if matches:\n",
    "        # Clean up and return the first match\n",
    "        value = matches[0].strip()\n",
    "        if after_colon:\n",
    "            # Remove any labels from the value\n",
    "            value = re.sub(r'^\\s*[:-]\\s*', '', value)\n",
    "        return value.strip()\n",
    "    return ''\n",
    "\n",
    "# Initialize the result dictionary\n",
    "result = {\n",
    "    \"Date\": \"\",  # Current date\n",
    "    \"Insured\": \"\",  # Name and address\n",
    "    \"Effective Start Date\": \"\",  # Today's date\n",
    "    \"Effective End Date\": \"\",  # One year from today's date\n",
    "    \"Deductible\": \"$5000\",  # Static value\n",
    "    \"Premium Rate\": \"\",  # Leave blank\n",
    "    \"Estimated Annual Payroll\": \"\",  # Total gross annual payroll\n",
    "    \"Policyholder/ Applicant’s Printed Name\": \"\"  # Name from the Insured\n",
    "}\n",
    "\n",
    "# Set current date\n",
    "current_date = datetime.now().date()\n",
    "result[\"Date\"] = current_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Set Effective Start Date\n",
    "result[\"Effective Start Date\"] = current_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Set Effective End Date (one year from today)\n",
    "effective_end_date = current_date + timedelta(days=365)\n",
    "result[\"Effective End Date\"] = effective_end_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Extract Insured Name and Address\n",
    "# We will extract lines after '1. Full Name of Insured:' and '2. Physical Address:'\n",
    "insured_name_field = ['1. Full Name of Insured', 'Full Name of Insured']\n",
    "insured_address_field = ['2. Physical Address', 'Physical Address']\n",
    "# Split the text into lines for easier processing\n",
    "lines = ocr_text.splitlines()\n",
    "\n",
    "# Extract Insured Name\n",
    "insured_name = ''\n",
    "insured_address = ''\n",
    "for i, line in enumerate(lines):\n",
    "    if any(field.lower() in line.lower() for field in insured_name_field):\n",
    "        # The name might be on the same line or the next line\n",
    "        if ':' in line:\n",
    "            insured_name = line.split(':', 1)[1].strip()\n",
    "            if not insured_name and i + 1 < len(lines):\n",
    "                insured_name = lines[i + 1].strip()\n",
    "        else:\n",
    "            if i + 1 < len(lines):\n",
    "                insured_name = lines[i + 1].strip()\n",
    "    if any(field.lower() in line.lower() for field in insured_address_field):\n",
    "        # The address might be on the same line or the next few lines\n",
    "        address_lines = []\n",
    "        for addr_line in lines[i+1:i+5]:  # We assume address is within the next 4 lines\n",
    "            if addr_line.strip():\n",
    "                address_lines.append(addr_line.strip())\n",
    "            else:\n",
    "                break\n",
    "        insured_address = ', '.join(address_lines)\n",
    "        break  # Assuming only one address to find\n",
    "\n",
    "if insured_name:\n",
    "    result[\"Insured\"] = insured_name + ', ' + insured_address\n",
    "    result[\"Policyholder/ Applicant’s Printed Name\"] = insured_name\n",
    "\n",
    "# Extract Estimated Annual Payroll\n",
    "payroll_field_names = [\n",
    "    'Total gross annual payroll',\n",
    "    'Total gross payroll',\n",
    "    'Estimated Annual Payroll',\n",
    "    'Total gross or annual payroll'\n",
    "]\n",
    "estimated_payroll = extract_field(ocr_text, payroll_field_names)\n",
    "# Clean up the payroll value by removing any non-digit characters except dot and comma\n",
    "estimated_payroll = re.sub(r'[^\\d.,]', '', estimated_payroll)\n",
    "# Normalize the number (remove commas)\n",
    "estimated_payroll = estimated_payroll.replace(',', '')\n",
    "result[\"Estimated Annual Payroll\"] = estimated_payroll\n",
    "\n",
    "# Now, output the result as JSON\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract json from MEL Application using Google Document AI - Candidate SOlution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Date\": \"2024-12-14\",\n",
      "  \"Insured\": \", Los Alamitos California 90720 3\",\n",
      "  \"Effective Start Date\": \"2024-12-14\",\n",
      "  \"Effective End Date\": \"2025-12-14\",\n",
      "  \"Deductible\": \"$5000\",\n",
      "  \"Premium Rate\": \"\",\n",
      "  \"Estimated Annual Payroll\": \"142000\",\n",
      "  \"Policyholder/ Applicant\\u2019s Printed Name\": \"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "\n",
    "# ====== Configuration ======\n",
    "\n",
    "# Path to your service account key JSON file\n",
    "service_key_path = \"/home/rhythm/Documents/Code/regenai/backend/credentials/regenai-service-key.json\"\n",
    "\n",
    "# Replace the following placeholders with your actual values\n",
    "project_id = \"regenai\"                       # Your Google Cloud project ID\n",
    "location = \"eu\"                              # Processor location, e.g., \"us\", \"eu\"\n",
    "processor_id = \"41265319209e4d75\"            # Your Document AI processor ID\n",
    "processor_version_id = None                  # Optional: Specify if using a specific processor version\n",
    "pdf_file_path = \"vtest11.pdf\"                 # Path to the PDF file you want to process\n",
    "\n",
    "# =============================\n",
    "\n",
    "# Step 1: Set the environment variable for Google Application Credentials\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = service_key_path\n",
    "\n",
    "# Step 2: Verify that the credentials file exists\n",
    "if not os.path.isfile(service_key_path):\n",
    "    raise FileNotFoundError(f\"Credentials file not found at: {service_key_path}\")\n",
    "\n",
    "# Step 3: Initialize the Document AI client\n",
    "client_options = {\"api_endpoint\": f\"{location}-documentai.googleapis.com\"}\n",
    "client = documentai.DocumentProcessorServiceClient(client_options=client_options)\n",
    "\n",
    "# Step 4: Define the processor name\n",
    "if processor_version_id:\n",
    "    # Full resource name of the processor version\n",
    "    processor_name = client.processor_version_path(project_id, location, processor_id, processor_version_id)\n",
    "else:\n",
    "    # Full resource name of the processor\n",
    "    processor_name = client.processor_path(project_id, location, processor_id)\n",
    "\n",
    "# Step 5: Verify that the PDF file exists\n",
    "if not os.path.isfile(pdf_file_path):\n",
    "    raise FileNotFoundError(f\"PDF file not found at: {pdf_file_path}\")\n",
    "\n",
    "# Step 6: Read the PDF file into memory\n",
    "with open(pdf_file_path, \"rb\") as file:\n",
    "    pdf_content = file.read()\n",
    "\n",
    "# Step 7: Create a RawDocument\n",
    "raw_document = documentai.RawDocument(content=pdf_content, mime_type=\"application/pdf\")\n",
    "\n",
    "# Step 8: Create the process request\n",
    "request = documentai.ProcessRequest(\n",
    "    name=processor_name,\n",
    "    raw_document=raw_document,\n",
    ")\n",
    "\n",
    "# Step 9: Process the document\n",
    "result = client.process_document(request=request)\n",
    "\n",
    "# Step 10: Retrieve the document object\n",
    "document = result.document\n",
    "\n",
    "# =============================\n",
    "# Extract text and parse required fields\n",
    "# =============================\n",
    "\n",
    "# Extract the full OCR text from the document\n",
    "ocr_text = document.text\n",
    "\n",
    "# Initialize the result dictionary\n",
    "result_data = {\n",
    "    \"Date\": \"\",  # Current date\n",
    "    \"Insured\": \"\",  # Name and address\n",
    "    \"Effective Start Date\": \"\",  # Today's date\n",
    "    \"Effective End Date\": \"\",  # One year from today's date\n",
    "    \"Deductible\": \"$5000\",  # Static value\n",
    "    \"Premium Rate\": \"\",  # Leave blank\n",
    "    \"Estimated Annual Payroll\": \"\",  # Total gross annual payroll\n",
    "    \"Policyholder/ Applicant’s Printed Name\": \"\"  # Name from the Insured\n",
    "}\n",
    "\n",
    "# Set current date\n",
    "current_date = datetime.now().date()\n",
    "result_data[\"Date\"] = current_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Set Effective Start Date\n",
    "result_data[\"Effective Start Date\"] = current_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Set Effective End Date (one year from today)\n",
    "effective_end_date = current_date + timedelta(days=365)\n",
    "result_data[\"Effective End Date\"] = effective_end_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define a function to extract information based on possible field names\n",
    "def extract_field(text, field_names, num_lines=1):\n",
    "    lines = text.splitlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        for field in field_names:\n",
    "            if field.lower() in line.lower():\n",
    "                # Extract the value(s) from the next 'num_lines' lines\n",
    "                value_lines = []\n",
    "                # Check if the value is on the same line after a colon\n",
    "                if ':' in line:\n",
    "                    value = line.split(':', 1)[1].strip()\n",
    "                    if value:\n",
    "                        value_lines.append(value)\n",
    "                # If not, get the next 'num_lines' lines\n",
    "                if not value_lines:\n",
    "                    for j in range(1, num_lines + 1):\n",
    "                        if i + j < len(lines):\n",
    "                            next_line = lines[i + j].strip()\n",
    "                            if next_line:\n",
    "                                value_lines.append(next_line)\n",
    "                            else:\n",
    "                                break\n",
    "                return ' '.join(value_lines).strip()\n",
    "    return ''\n",
    "\n",
    "# Extract Insured Name and Address\n",
    "insured_name_field = [\n",
    "    '1. Full Name of Insured',\n",
    "    'Full Name of Insured',\n",
    "    'Name of Insured',\n",
    "    'Insured Name'\n",
    "]\n",
    "insured_address_field = [\n",
    "    '2. Physical Address',\n",
    "    'Physical Address',\n",
    "    'Address'\n",
    "]\n",
    "\n",
    "# Extract Insured Name\n",
    "insured_name = extract_field(ocr_text, insured_name_field)\n",
    "result_data[\"Insured\"] = insured_name\n",
    "result_data[\"Policyholder/ Applicant’s Printed Name\"] = insured_name\n",
    "\n",
    "# Extract Insured Address (assume address spans up to 4 lines)\n",
    "insured_address = extract_field(ocr_text, insured_address_field, num_lines=4)\n",
    "if insured_address:\n",
    "    result_data[\"Insured\"] += ', ' + insured_address\n",
    "\n",
    "# Extract Estimated Annual Payroll\n",
    "payroll_field_names = [\n",
    "    'Total gross annual payroll',\n",
    "    'Total gross payroll',\n",
    "    'Estimated Annual Payroll',\n",
    "    'Total gross or annual payroll',\n",
    "    'Total All'\n",
    "]\n",
    "estimated_payroll = extract_field(ocr_text, payroll_field_names)\n",
    "# Clean up the payroll value by removing any non-digit characters except dot and comma\n",
    "estimated_payroll = re.sub(r'[^\\d.,]', '', estimated_payroll)\n",
    "# Normalize the number (remove commas)\n",
    "estimated_payroll = estimated_payroll.replace(',', '')\n",
    "result_data[\"Estimated Annual Payroll\"] = estimated_payroll\n",
    "\n",
    "# Output the result as JSON\n",
    "print(json.dumps(result_data, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regenai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
